# AI for cyber security 2022/23
# Authors:
# - Cirillo Benedetto 
# - Montervino Dario 
# - Salzano Simone
# - Tisi Andrea

import os
import pickle
import torch.nn as nn
import torch
import torch.optim as optim
from torchvision import transforms
from torchvision.models import resnet
# imported by us
from argparse import ArgumentParser
from utils import cnn_training
from torch.utils.data import DataLoader
from utils.BinToImg import BinToImg
from utils.TarDataset import TarDataset
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report


def main():
  #---------------------------------- DEFINITION OF THE CLASSES ----------------------------------------
  parser = ArgumentParser()
  parser.add_argument('--batch_size', help='The batch size', type=int, default=256)
  parser.add_argument('--model_save_path', help='Folder where the model will be saved', type=str, default=None)
  parser.add_argument('--n_epochs', help='Number of epochs in the training phase', type=int, default=50)
  parser.add_argument('--early_stopping', help='Max number of epochs without an improvement on the validation_accuracy', type=int, default=0)
  parser.add_argument('--file_dim', help='Max dim of the input file for the model (in MB)', type=int, default=2)
  parser.add_argument('--verbose', help='Show the progress during the training phase', type=int, default=1)
  parser.add_argument('--dataset_path', help='Dataset Tar Location (no GZ compression)', type=str, default=None)
  parser.add_argument('--model_eval_path', help='Folder where the model will be taken for evaulate it', type=str, default=None)
  parser.add_argument('--eval_set', help='Which dataset is going to be evaluated', type=str, default=None)
  parser.add_argument('--metrics_folder', help='Path where the metrics of the model will be stored',type=str, default=None)
  args = parser.parse_args()
  print("\nModel paramteres:\n")
  print("Batch size: ",args.batch_size)
  print("Folder where the model will be saved: ",args.model_save_path)
  print("Folder where the model will be taken: ",args.model_eval_path)
  print("Number of epochs in the training phase: ",args.n_epochs)
  print("Verbose: ", args.verbose==1)
  print("\n")
  
  if args.file_dim == 2:
    file_dim = 2**21
  else:
    file_dim = 2**20
  print("Verbose: ", args.verbose == 1)
  print("\n")
  
  # Selecting the device 
  device=cnn_training.get_device()
  if args.verbose==1:
    print("Is the GPU available? ", torch.cuda.is_available() or torch.backends.mps.is_available())

  torch.manual_seed(666)

  img_transform=BinToImg()
  data_transforms_bin = transforms.Compose([
    img_transform,
    transforms.Grayscale(num_output_channels=3),
    transforms.Resize((224,224)), 
    transforms.ToTensor()
    ])

  #---------------------------------- PREPARATION OF THE MODEL ----------------------------------------
  model = resnet.resnet50(pretrained=True)
  # Replacing the last fully-connected layer
  model.fc = nn.Sequential(
              nn.Linear(2048, 128),
              nn.ReLU(inplace=True),
              nn.Linear(128, 1), #2 is the number of classes
              nn.Sigmoid())

  criterion = nn.BCELoss()
  optimizer = optim.Adam(model.fc.parameters())

  if args.model_save_path is not None:

    #---------------------------------- PREPARATION OF THE DATASET ----------------------------------------
    
    file_list_train = cnn_training.get_file_list(args.dataset_path, 'train')
    dataset = TarDataset(path_tar=args.dataset_path,member_list=file_list_train,first_n_byte=file_dim, padding=False, transform=data_transforms_bin)
    train_size=len(dataset)-round(len(dataset)/5)
    test_size=round(len(dataset)/5)
    train_set, val_set =  torch.utils.data.random_split(dataset, [train_size, test_size])
    print(f"Train size:{len(train_set)}\nValidation size:{len(val_set)}")
    train_loader = DataLoader(train_set, batch_size=args.batch_size, num_workers=1, shuffle=True)
    val_loader = DataLoader(val_set, batch_size=args.batch_size, num_workers=1, shuffle=True)
    dataloader={
      'train': train_loader,
      'validation': val_loader
    }


  #---------------------------------- TRAINING PHASE ----------------------------------------
    cnn_training.train_model(model, criterion, optimizer, device, dataloader,args.n_epochs,  args.model_save_path, early_stopping=args.early_stopping )


#---------------------------------- TEST PHASE ----------------------------------------
  elif args.model_eval_path is not None:

    file_list_test = cnn_training.get_file_list(args.dataset_path, args.eval_set)
    train_set = TarDataset(path_tar=args.dataset_path,member_list=file_list_test,first_n_byte=file_dim, padding=False, transform=data_transforms_bin)
    test_loader=DataLoader(train_set, batch_size=args.batch_size, num_workers=1, shuffle=True)
    print(f"Train Set Length:{len(train_set)}")

    # model=torch.load(args.model_eval_path)
    model.load_state_dict(torch.load(args.model_eval_path))
    model.eval()
    criterion =nn.BCELoss()
    outputs,labels = cnn_training.model_evaluate(model, test_loader, device, criterion, args.verbose)
    
    conf = confusion_matrix(labels, outputs, normalize='true')
    classification = classification_report(labels, outputs)
    accuracy = accuracy_score(labels, outputs)
    print("Normalized confusion matrix \n %s" % (conf))
    print("Classification report\n %s" % (classification))
    print("Accuracy score: %.3f" % (accuracy))
          
    if args.metrics_folder is not None:
          with open(f"{args.metrics_folder}/imagebased/{args.eval_set}_confusion_matrix.pkl","wb") as fh:
              pickle.dump(conf, fh)
          with open(f"{args.metrics_folder}/imagebased/{args.eval_set}_classification_report.pkl","wb") as fh:
              pickle.dump(classification, fh)               
          with open(f"{args.metrics_folder}/imagebased/{args.eval_set}_accuracy_score.pkl","wb") as fh:
              pickle.dump(accuracy, fh)
              
              
if __name__ == '__main__':
  main()