import numpy as np
import torch
import sys
from PIL import Image
from torch.utils.data import TensorDataset, DataLoader, random_split
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, models, transforms
from torch.utils.data import Dataset
# from torchsummary import summary #network summary
from torchvision.models import resnet50, ResNet50_Weights
import matplotlib.pyplot as plt
import tarfile
import PIL
from argparse import ArgumentParser
from utils.BinToImg import BinToImg
from utils.ImageDataset import ImageDataset
import time
from utils import cnn_training
import os



parser = ArgumentParser()
parser.add_argument('--batch_size', help='The batch size', type=int, default=256)
parser.add_argument('--model_save_path', help='Folder where the model will be saved', type=str, default=None)
parser.add_argument('--model_eval_path', help='Folder where the model will be taken for evaulate it', type=str, default=None)
parser.add_argument('--perc_validation', help='Percentage of the validation set compared to the total dataset used for training', type=float, default=0.3)
parser.add_argument('--n_epochs', help='Number of epochs in the training phase', type=int, default=50)
parser.add_argument('--early_stopping', help='Max number of epochs without an improvement on the validation_accuracy', type=int, default=0)
parser.add_argument('--file_dim', help='Max dim of the input file for the model (in MB)', type=int, default=2)
parser.add_argument('--verbose', help='Show the progress during the training phase', type=int, default=1)
parser.add_argument('--dataset_path', help='Dataset Tar Location (no GZ compression)', type=str, default=None)
args = parser.parse_args()
print("\nModel paramteres:\n")
print("Batch size: ",args.batch_size)
print("Folder where the model will be saved: ",args.model_save_path)
print("Folder where the model will be taken: ",args.model_eval_path)
print("Percentage of the validation set compared to the total dataset used for training: ",args.perc_validation)
print("Number of epochs in the training phase: ",args.n_epochs)


file_list_train=[]
tar = tarfile.open(args.dataset_path,"r")
for member in tar.getmembers():
    info_member=member.get_info()
    if info_member['size']>0:   #Ã¨ un file e non una dir
      name_splitted=info_member['name'].split('/')
      train_or_test=name_splitted[-3]
      malware_type=name_splitted[-2]
      if(train_or_test=='train'):
          if(malware_type=='benign'):
              file_list_train.append((member,0))
          else:
              file_list_train.append((member,1))

tar.close()


target_transform = transforms.Lambda(lambda x: BinToImg().forward(x))
data_transforms_bin = transforms.Compose([
    target_transform,
    transforms.Grayscale(num_output_channels=3),
    transforms.Resize((224,224)), 
    transforms.ToTensor()
    ])
train_dataset = ImageDataset(path_tar=args.dataset_path,member_list=file_list_train,first_n_byte=2000000, transform=data_transforms_bin)
dataset_length=len(train_dataset)
train_data, val_data = random_split(train_dataset, [round(dataset_length*(1-args.perc_validation)), round(dataset_length*args.perc_validation)])
print("The dataset length is : ",dataset_length)
print("The train_dataset length is : ",round(dataset_length*(1-args.perc_validation)))
print("The valid_dataset length is : ",round(dataset_length*args.perc_validation))



train_loader=torch.utils.data.DataLoader(train_data,batch_size=args.batch_size),
val_loader=torch.utils.data.DataLoader(val_data, batch_size=args.batch_size), 



##-----------------------MODEL--------------
if torch.backends.mps.is_available():
    device=torch.device('mps')
elif torch.cuda.is_available():
    device=torch.device('cuda:0')
else:
    device=torch.device('cpu')
model = models.resnet50(weights=ResNet50_Weights.DEFAULT).to(device)
# Replacing the last fully-connected layer
model.fc = nn.Sequential(
               nn.Linear(2048, 128),
               nn.ReLU(inplace=True),
               nn.Linear(128, 2), #2 is the number of classes
               nn.Softmax(dim=1)).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.fc.parameters())

if args.model_save_path is not None:
    train_loss = []
    train_acc = []
    val_loss = []
    val_acc = []
    st = time.time()
    i=0
    max=0
    early_stop_i=0
    for epoch in range(int(args.n_epochs)):
        st_epoch = time.time()
        print("\n\n##################### Epoch {} #####################".format(epoch))
        train_loss, train_acc = cnn_training.train_model(model=model, device=device, phase='train', dataloaders=train_loader, criterion=criterion, optimizer=optimizer,loss_arr=train_loss, acc_arr=train_acc,verbose=args.verbose)
        print("\nValidation\n")
        val_loss, val_acc = cnn_training.train_model(model=model, device=device, phase='validation', dataloaders=train_loader, criterion=criterion, optimizer=optimizer,loss_arr=val_loss, acc_arr=val_acc,verbose=args.verbose)
        print("\n\nTrain_loss: {:.3f}, Train_acc: {:.3f}".format(np.mean(train_loss), np.mean(train_acc)))
        print("Validation_loss: {:.3f}, Validation_acc: {:.3f}".format(np.mean(val_loss), np.mean(val_acc)))
        et_epoch = time.time()
        print('Execution time for epoch ', i,' :', et_epoch-st_epoch, 'seconds')

        if np.mean(val_acc)>=max:
          max=np.mean(val_acc)
          early_stop_i=0
          torch.save(model, os.path.join(args.model_save_path,"best_model.pth.tar"))
          print("Best model saved")
        else:
          early_stop_i+=1
        if early_stop_i==args.early_stopping:
          print("EARLY STOPPING")
          break
    et = time.time()
    elapsed_time = et - st
    print('Execution time:', elapsed_time, 'seconds')

    #Save the model
    torch.save(model, os.path.join(args.model_save_path,"model.pth.tar"))

  #---------------------------------- EVALUATION PHASE ----------------------------------------
if args.model_eval_path is not None:
#Load the models
    model = torch.load(args.model_eval_path)
    model.eval()


