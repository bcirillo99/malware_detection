# AI for cyber security 2022/23
# Authors:
# - Cirillo Benedetto 
# - Montervino Dario 
# - Salzano Simone
# - Tisi Andrea

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, models, transforms
from torchsummary import summary #network summary

# imported by us
import os
from torch.utils.data import Dataset
from argparse import ArgumentParser
import time
from model.MalConv import MalConv
from utils.MalDataset import MalDataset

#---------------------------------- DEFINITION OF THE CLASSES ----------------------------------------


# Training function
def train(model, device, dataloader, loss_fn, optimizer, verbose):
    train_loss = []
    train_acc = []
    model.train()
    i=1
    for file_batch, labels in dataloader:
        #Computing the loss
        file_batch, labels = file_batch.to(device), labels.to(device)
        print("Compute batch {:d}/{:d}:".format(i,len(dataloader)))
        output = model(file_batch)
        loss = loss_fn(output, labels)
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_acc.append(list(labels.detach().cpu().numpy().astype(int)==np.around(output.detach().cpu().numpy()).astype(int)))
        train_loss.append(loss.detach().cpu().numpy())
        if verbose==1:
          print("Train_loss: {:.3f}, Train_acc: {:.3f}".format(np.mean(train_loss), np.mean(train_acc)))
        i=i+1
    return np.mean(train_loss),np.mean(train_acc)

# Test function
def test(model, device, dataloader, loss_fn, optimizer, verbose):
    test_loss = []
    test_acc = []
    model.eval()
    with torch.no_grad(): #Disabling gradient calculation of the context manager
        for file_batch, labels in dataloader:
            output = model(file_batch)
            loss = loss_fn(output, labels)
            #detaching the tensor from gradient computation and returns a copy of this object in CPU memory.
            test_acc.append(list(labels.detach().cpu().numpy().astype(int)==np.around(output.detach().cpu().numpy()).astype(int)))
            test_loss.append(loss.detach().cpu().numpy()) 
            if verbose==1:
              print("Val_loss: {:.3f}, val_acc: {:.3f}".format(np.mean(test_loss), np.mean(test_acc)))
    return np.mean(test_loss), np.mean(test_acc)



def main():
  parser = ArgumentParser()
  parser.add_argument('--batch_size', help='The batch size', type=int, default=256)
  parser.add_argument('--model_save_path', help='Folder where the model will be saved', type=str, default=None)
  parser.add_argument('--model_eval_path', help='Folder where the model will be taken for evaulate it', type=str, default=None)
  parser.add_argument('--perc_validation', help='Percentage of the validation set compared to the total dataset used for training', type=float, default=0.3)
  parser.add_argument('--n_epochs', help='Number of epochs in the training phase', type=int, default=50)
  parser.add_argument('--early_stopping', help='Max number of epochs without an improvement on the validation_accuracy', type=int, default=0)
  parser.add_argument('--file_dim', help='Max dim of the input file for the model (in MB)', type=int, default=2)
  parser.add_argument('--verbose', help='Show the progress during the training phase', type=int, default=1)
  args = parser.parse_args()
  print("\nModel paramteres:\n")
  print("Batch size: ",args.batch_size)
  print("Folder where the model will be saved: ",args.model_save_path)
  print("Folder where the model will be taken: ",args.model_eval_path)
  print("Percentage of the validation set compared to the total dataset used for training: ",args.perc_validation)
  print("Number of epochs in the training phase: ",args.n_epochs)
  if args.early_stopping==0:
    early_stopping=args.n_epochs
  else:
    early_stopping=args.early_stopping
  print("Early stopping on the val accuracy: ",early_stopping)
  if args.file_dim==2:
    file_dim=2**21
    print("Model: MalConv (file dim of ",file_dim," byte)")
  else:
    file_dim=2**20
    print("Model: emberMalConv (file dim of ",file_dim," byte)")
  print("Verbose: ", args.verbose==1)


  
  print("\n")

  #---------------------------------- PREPARATION OF THE DATASET ----------------------------------------
  file_list=[]
  for root, dirs, files in os.walk("train/", topdown=False):
      for name in files:
        path = os.path.join(root, name)
        if "DS_Store" not in path:
          folder = path.split(os.sep)[-2]
          if folder=="benign":
            file_list.append((path,float(0)))
          else:
            file_list.append((path,float(1)))

  train_dataset = MalDataset(fp_list=file_list,first_n_byte=file_dim)

  train_transform = transforms.Compose([
  transforms.ToTensor(),
  ])
  train_dataset.transform = train_transform

  """test_transform = transforms.Compose([
  transforms.ToTensor(),
  ])
  test_dataset.transform = test_transform"""

  perc_split=args.perc_validation
  dataset_length=len(train_dataset)
  train_data, val_data = random_split(train_dataset, [round(dataset_length*(1-perc_split)), round(dataset_length*perc_split)])
  print("The dataset length is : ",dataset_length)
  print("The train_dataset length is : ",round(dataset_length*(1-perc_split)))
  print("The valid_dataset length is : ",round(dataset_length*perc_split))
  batch_size=args.batch_size

  train_loader = DataLoader(train_data, batch_size=batch_size,num_workers=1)
  valid_loader = DataLoader(val_data, batch_size=batch_size,num_workers=1)
  # test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=True)

  #---------------------------------- PREPARATION OF THE MODEL ----------------------------------------
  MalConv_net=MalConv(input_length=file_dim)

  n_epochs = args.n_epochs
  criterion =nn.BCELoss()
  optimizer = optim.Adam(MalConv_net.parameters())
  # Selecting the device on apple silicon
  if torch.backends.mps.is_available():
    device=torch.device('mps')
  elif torch.backends.cuda.is_available():
    device=torch.device('cuda')
  else:
    device=torch.device('cpu')

  print(f"Using device: {device}")
  MalConv_net.to(device)
  criterion.to(device)
  print("Is the model on the GPU? ",next(MalConv_net.parameters()).is_mps)

  #---------------------------------- TRAINING PHASE ----------------------------------------
  if args.model_save_path is not None:
    st = time.time()
    i=0
    max=0
    early_stop_i=0
    for epoch in range(n_epochs):
        st_epoch = time.time()
        print("\n\n##################### Epoch {} #####################".format(epoch))
        train_loss, train_acc = train(MalConv_net,device,train_loader,criterion,optimizer,args.verbose)
        val_loss, val_acc = test(MalConv_net,device,valid_loader,criterion,optimizer,args.verbose)
        print("Train_loss: {:.3f}, Train_acc: {:.3f}".format(train_loss, train_acc))
        print("Validation_loss: {:.3f}, Validation_acc: {:.3f}".format(val_loss, val_acc))
        et_epoch = time.time()
        print('Execution time for epoch ', i,' :', et_epoch-st_epoch, 'seconds')

        if val_acc>=max:
          max=val_acc
          early_stop_i=0
          torch.save(MalConv_net, args.model_save_path+"/best_model.pt")
          print("Best model saved")
        else:
          early_stop_i+=1
        if early_stop_i==early_stopping:
          print("EARLY STOPPING")
          break
    et = time.time()
    elapsed_time = et - st
    print('Execution time:', elapsed_time, 'seconds')

    #Save the model
    torch.save(MalConv_net, args.model_save_path+"/model.pt")

  #---------------------------------- EVALUATION PHASE ----------------------------------------
  if args.model_eval_path is not None:
    #Load the models
    
    model = torch.load(args.model_eval_path)
    model.eval()

if __name__ == '__main__':
    main()